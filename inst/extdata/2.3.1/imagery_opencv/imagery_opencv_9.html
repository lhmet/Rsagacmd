<h4>Tool</h4><table border="0"><tr><td valign="top"><b>Name</b></td><td valign="top">Boosting Classification (OpenCV)</td></tr><tr><td valign="top"><b>ID</b></td><td valign="top">9</td></tr><tr><td valign="top"><b>Author</b></td><td valign="top">O.Conrad (c) 2016</td></tr><tr><td valign="top"><b>Specification</b></td><td valign="top">grid</td></tr></table><hr><h4>Description</h4>Integration of the OpenCV Machine Learning library for Boosted Trees classification of gridded features.
<a href="http://docs.opencv.org">Open Source Computer Vision</a><hr><h4>Parameters</h4><table border="1" width="100%" valign="top" cellpadding="5" rules="all"><tr><th>Name</th><th>Type</th><th>Identifier</th><th>Description</th><th>Constraints</th></tr>
<tr><th colspan="5">Input</th></tr><tr><td>Features </td><td>Grid list (input)</td><td>FEATURES</td><td></td><td></td></tr><tr><td>Training Areas </td><td>Shapes (input)</td><td>TRAIN_AREAS</td><td></td><td></td></tr><tr><th colspan="5">Output</th></tr><tr><td>Classification</td><td>Grid (output)</td><td>CLASSES</td><td></td><td></td></tr><tr><th colspan="5">Options</th></tr><tr><td>Normalize</td><td>Boolean</td><td>NORMALIZE</td><td></td><td>Default: 0</td></tr><tr><td>Class Identifier</td><td>Table field</td><td>TRAIN_CLASS</td><td></td><td></td></tr><tr><td>Maximum Tree Depth</td><td>Integer</td><td>MAX_DEPTH</td><td>The maximum possible depth of the tree. That is the training algorithms attempts to split a node while its depth is less than maxDepth. The root node has zero depth.</td><td>Minimum: 1
Default: 10</td></tr><tr><td>Minimum Sample Count</td><td>Integer</td><td>MIN_SAMPLES</td><td>If the number of samples in a node is less than this parameter then the node will not be split.</td><td>Minimum: 2
Default: 2</td></tr><tr><td>Maximum Categories</td><td>Integer</td><td>MAX_CATEGRS</td><td>Cluster possible values of a categorical variable into K<=maxCategories clusters to find a suboptimal split.</td><td>Minimum: 1
Default: 10</td></tr><tr><td>Use 1SE Rule</td><td>Boolean</td><td>1SE_RULE</td><td>If true then a pruning will be harsher. This will make a tree more compact and more resistant to the training data noise but a bit less accurate.</td><td>Default: 1</td></tr><tr><td>Truncate Pruned Trees</td><td>Boolean</td><td>TRUNC_PRUNED</td><td>If true then pruned branches are physically removed from the tree. Otherwise they are retained and it is possible to get results from the original unpruned (or pruned less aggressively) tree.</td><td>Default: 1</td></tr><tr><td>Regression Accuracy</td><td>Floating point</td><td>REG_ACCURACY</td><td>Termination criteria for regression trees. If all absolute differences between an estimated value in a node and values of train samples in this node are less than this parameter then the node will not be split further.</td><td>Minimum: 0.000000
Default: 0.010000</td></tr><tr><td>Weak Count</td><td>Integer</td><td>WEAK_COUNT</td><td>The number of weak classifiers.</td><td>Minimum: 0
Default: 100</td></tr><tr><td>Weight Trim Rate</td><td>Floating point</td><td>WGT_TRIM_RATE</td><td>A threshold between 0 and 1 used to save computational time. Set this parameter to 0 to turn off this functionality.</td><td>Minimum: 0.000000
Maximum: 1.000000
Default: 0.950000</td></tr><tr><td>Boost Type</td><td>Choice</td><td>BOOST_TYPE</td><td></td><td>Available Choices:
[0] Discrete AdaBoost
[1] Real AdaBoost
[2] LogitBoost
[3] Gentle AdaBoost
Default: 1</td></tr></table>